{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true,
      "authorship_tag": "ABX9TyMim6/ZrTrbguBMOPVR14Wh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DSGP-Group-1-EAPS/BackupDataset/blob/main/DSGP_Coursework_EAPS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIjlRLHsT5LZ",
        "outputId": "2840befc-1424-4a3e-8795-353df5afe62e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Epoch 1/50\n",
            "19/19 - 1s - loss: 234.5580 - val_loss: 134.0117 - 1s/epoch - 57ms/step\n",
            "Epoch 2/50\n",
            "19/19 - 0s - loss: 222.1121 - val_loss: 123.5464 - 87ms/epoch - 5ms/step\n",
            "Epoch 3/50\n",
            "19/19 - 0s - loss: 209.3505 - val_loss: 113.6572 - 70ms/epoch - 4ms/step\n",
            "Epoch 4/50\n",
            "19/19 - 0s - loss: 197.6781 - val_loss: 106.9081 - 85ms/epoch - 4ms/step\n",
            "Epoch 5/50\n",
            "19/19 - 0s - loss: 190.1823 - val_loss: 104.2495 - 80ms/epoch - 4ms/step\n",
            "Epoch 6/50\n",
            "19/19 - 0s - loss: 186.3644 - val_loss: 103.0893 - 79ms/epoch - 4ms/step\n",
            "Epoch 7/50\n",
            "19/19 - 0s - loss: 183.6711 - val_loss: 102.5706 - 87ms/epoch - 5ms/step\n",
            "Epoch 8/50\n",
            "19/19 - 0s - loss: 181.5339 - val_loss: 101.8148 - 83ms/epoch - 4ms/step\n",
            "Epoch 9/50\n",
            "19/19 - 0s - loss: 180.0308 - val_loss: 101.6352 - 69ms/epoch - 4ms/step\n",
            "Epoch 10/50\n",
            "19/19 - 0s - loss: 178.8548 - val_loss: 102.2256 - 84ms/epoch - 4ms/step\n",
            "Epoch 11/50\n",
            "19/19 - 0s - loss: 177.2756 - val_loss: 101.4305 - 87ms/epoch - 5ms/step\n",
            "Epoch 12/50\n",
            "19/19 - 0s - loss: 175.7465 - val_loss: 101.5556 - 81ms/epoch - 4ms/step\n",
            "Epoch 13/50\n",
            "19/19 - 0s - loss: 174.7460 - val_loss: 101.9278 - 78ms/epoch - 4ms/step\n",
            "Epoch 14/50\n",
            "19/19 - 0s - loss: 174.3559 - val_loss: 102.3282 - 69ms/epoch - 4ms/step\n",
            "Epoch 15/50\n",
            "19/19 - 0s - loss: 172.7221 - val_loss: 101.5215 - 79ms/epoch - 4ms/step\n",
            "Epoch 16/50\n",
            "19/19 - 0s - loss: 172.1747 - val_loss: 101.8435 - 69ms/epoch - 4ms/step\n",
            "Epoch 17/50\n",
            "19/19 - 0s - loss: 170.7345 - val_loss: 101.5474 - 66ms/epoch - 3ms/step\n",
            "Epoch 18/50\n",
            "19/19 - 0s - loss: 169.7721 - val_loss: 101.6137 - 65ms/epoch - 3ms/step\n",
            "Epoch 19/50\n",
            "19/19 - 0s - loss: 168.8443 - val_loss: 101.4444 - 72ms/epoch - 4ms/step\n",
            "Epoch 20/50\n",
            "19/19 - 0s - loss: 168.0509 - val_loss: 101.2728 - 68ms/epoch - 4ms/step\n",
            "Epoch 21/50\n",
            "19/19 - 0s - loss: 166.6450 - val_loss: 102.2375 - 86ms/epoch - 5ms/step\n",
            "Epoch 22/50\n",
            "19/19 - 0s - loss: 166.2122 - val_loss: 101.9955 - 85ms/epoch - 4ms/step\n",
            "Epoch 23/50\n",
            "19/19 - 0s - loss: 165.1183 - val_loss: 102.1484 - 68ms/epoch - 4ms/step\n",
            "Epoch 24/50\n",
            "19/19 - 0s - loss: 164.8091 - val_loss: 104.2285 - 74ms/epoch - 4ms/step\n",
            "Epoch 25/50\n",
            "19/19 - 0s - loss: 163.7483 - val_loss: 102.4056 - 66ms/epoch - 3ms/step\n",
            "Epoch 26/50\n",
            "19/19 - 0s - loss: 161.9910 - val_loss: 105.0048 - 77ms/epoch - 4ms/step\n",
            "Epoch 27/50\n",
            "19/19 - 0s - loss: 161.2059 - val_loss: 103.8779 - 71ms/epoch - 4ms/step\n",
            "Epoch 28/50\n",
            "19/19 - 0s - loss: 159.1977 - val_loss: 104.4660 - 80ms/epoch - 4ms/step\n",
            "Epoch 29/50\n",
            "19/19 - 0s - loss: 157.8788 - val_loss: 104.8920 - 84ms/epoch - 4ms/step\n",
            "Epoch 30/50\n",
            "19/19 - 0s - loss: 157.3403 - val_loss: 104.7301 - 75ms/epoch - 4ms/step\n",
            "Epoch 31/50\n",
            "19/19 - 0s - loss: 156.1743 - val_loss: 104.8712 - 73ms/epoch - 4ms/step\n",
            "Epoch 32/50\n",
            "19/19 - 0s - loss: 154.6350 - val_loss: 106.5158 - 67ms/epoch - 4ms/step\n",
            "Epoch 33/50\n",
            "19/19 - 0s - loss: 153.4394 - val_loss: 107.2297 - 65ms/epoch - 3ms/step\n",
            "Epoch 34/50\n",
            "19/19 - 0s - loss: 153.1271 - val_loss: 107.1902 - 71ms/epoch - 4ms/step\n",
            "Epoch 35/50\n",
            "19/19 - 0s - loss: 151.0966 - val_loss: 109.1310 - 69ms/epoch - 4ms/step\n",
            "Epoch 36/50\n",
            "19/19 - 0s - loss: 150.3609 - val_loss: 108.0515 - 64ms/epoch - 3ms/step\n",
            "Epoch 37/50\n",
            "19/19 - 0s - loss: 148.8858 - val_loss: 109.8398 - 77ms/epoch - 4ms/step\n",
            "Epoch 38/50\n",
            "19/19 - 0s - loss: 147.5680 - val_loss: 110.0858 - 84ms/epoch - 4ms/step\n",
            "Epoch 39/50\n",
            "19/19 - 0s - loss: 147.6333 - val_loss: 109.5912 - 81ms/epoch - 4ms/step\n",
            "Epoch 40/50\n",
            "19/19 - 0s - loss: 145.5554 - val_loss: 112.0732 - 81ms/epoch - 4ms/step\n",
            "Epoch 41/50\n",
            "19/19 - 0s - loss: 145.0852 - val_loss: 112.3558 - 71ms/epoch - 4ms/step\n",
            "Epoch 42/50\n",
            "19/19 - 0s - loss: 143.1654 - val_loss: 113.7418 - 81ms/epoch - 4ms/step\n",
            "Epoch 43/50\n",
            "19/19 - 0s - loss: 142.7638 - val_loss: 114.5398 - 74ms/epoch - 4ms/step\n",
            "Epoch 44/50\n",
            "19/19 - 0s - loss: 141.7684 - val_loss: 115.0459 - 85ms/epoch - 4ms/step\n",
            "Epoch 45/50\n",
            "19/19 - 0s - loss: 140.4487 - val_loss: 116.5130 - 68ms/epoch - 4ms/step\n",
            "Epoch 46/50\n",
            "19/19 - 0s - loss: 140.2121 - val_loss: 117.5872 - 65ms/epoch - 3ms/step\n",
            "Epoch 47/50\n",
            "19/19 - 0s - loss: 138.9806 - val_loss: 118.6487 - 85ms/epoch - 4ms/step\n",
            "Epoch 48/50\n",
            "19/19 - 0s - loss: 137.4041 - val_loss: 118.9370 - 80ms/epoch - 4ms/step\n",
            "Epoch 49/50\n",
            "19/19 - 0s - loss: 136.4209 - val_loss: 120.4250 - 85ms/epoch - 4ms/step\n",
            "Epoch 50/50\n",
            "19/19 - 0s - loss: 135.9004 - val_loss: 120.7190 - 86ms/epoch - 5ms/step\n",
            "19/19 [==============================] - 0s 3ms/step\n",
            "5/5 [==============================] - 0s 4ms/step\n",
            "[[  7.   3.   1. ...  90. 172.  30.]\n",
            " [  7.   3.   1. ...  98. 178.  31.]\n",
            " [  7.   4.   1. ...  89. 170.  31.]\n",
            " ...\n",
            " [  0.   3.   1. ...  98. 170.  34.]\n",
            " [  0.   4.   2. ... 100. 170.  35.]\n",
            " [  0.   6.   3. ...  77. 175.  25.]]\n",
            "[  4.           0.           2.           4.           2.\n",
            "   6.97771588   8.           4.          40.           8.\n",
            "   8.           8.           8.           1.           4.\n",
            "   6.97771588   2.           8.           8.           2.\n",
            "   6.97771588   1.          40.           4.           8.\n",
            "   7.           1.           4.           8.           2.\n",
            "   8.           8.           4.           8.           2.\n",
            "   1.           8.           4.           8.           4.\n",
            "   2.           4.           4.           8.           2.\n",
            "   3.           3.           4.           6.97771588  32.\n",
            "   6.97771588   0.           2.           2.           0.\n",
            "   6.97771588   3.           3.           0.           1.\n",
            "   3.           4.           3.           6.97771588   6.97771588\n",
            "   1.           3.           3.           3.           2.\n",
            "   2.           5.           8.           3.          16.\n",
            "   8.           2.           8.           1.           3.\n",
            "   1.           1.           8.           8.           5.\n",
            "  32.           8.          40.           1.           8.\n",
            "   3.           8.           3.           4.           1.\n",
            "   3.          24.           3.           1.          64.\n",
            "   2.           8.           2.           8.          56.\n",
            "   8.           3.           3.           2.           8.\n",
            "   2.           8.           2.           1.           1.\n",
            "   1.           8.           2.           2.           2.\n",
            "   1.           2.           2.           2.           2.\n",
            "   2.           2.           2.           2.           8.\n",
            "   8.           2.           2.           2.           0.\n",
            "   1.           3.           1.           8.           8.\n",
            "   2.           8.           2.           8.           8.\n",
            "   8.           2.           2.           1.           8.\n",
            "   3.           8.           1.           1.           8.\n",
            "   2.           8.           3.           8.           8.\n",
            "   8.           8.           3.          40.          40.\n",
            "  16.          16.           8.           8.           8.\n",
            "   4.           1.           8.          24.           2.\n",
            "   8.           1.           8.          16.           3.\n",
            "  16.           2.           3.           1.           1.\n",
            "   1.           1.          24.           1.           2.\n",
            "   4.          24.           1.           3.           8.\n",
            "   1.           8.          56.           8.          24.\n",
            "   8.          16.           3.           0.           8.\n",
            "   2.           1.           8.           8.           4.\n",
            "   2.           1.          24.           0.           0.\n",
            "   6.97771588   0.           1.          24.           6.97771588\n",
            "   8.           8.          24.           4.           6.97771588\n",
            "   8.           4.           8.           8.          16.\n",
            "   1.          80.           8.           2.           2.\n",
            "   2.          16.           8.           8.           4.\n",
            "   8.           8.           2.           8.           8.\n",
            "   3.           8.           8.           8.          32.\n",
            "   8.           0.           8.           3.           1.\n",
            "   8.           1.           2.           4.           4.\n",
            "   1.           8.           1.           3.           2.\n",
            "   1.           1.           8.           8.           6.97771588\n",
            "   8.           3.          24.           0.          16.\n",
            "   3.           0.           0.           8.          32.\n",
            "   1.           4.           4.           8.           1.\n",
            "   0.           3.          40.           8.           8.\n",
            "   4.           8.           8.           0.           0.\n",
            "   8.           3.           8.           1.          64.\n",
            "   0.          16.           6.97771588   0.           2.\n",
            "   2.           1.           4.          16.           1.\n",
            "   8.           0.           0.           6.97771588   5.\n",
            "   5.           1.           8.           2.           8.\n",
            "   3.           1.           8.           6.97771588   8.\n",
            "   0.           1.           3.           2.           3.\n",
            "   8.           4.           8.           1.           8.\n",
            "   8.           0.         120.           1.           3.\n",
            "   2.           1.           3.           1.           4.\n",
            "   8.           1.           1.           1.           8.\n",
            "   2.           1.           8.           4.           8.\n",
            "   2.           3.           8.           5.          32.\n",
            "   2.           1.           4.           8.           8.\n",
            "   8.           4.           1.           1.           2.\n",
            "   3.           1.           3.           3.           3.\n",
            "   2.           3.           8.           8.           3.\n",
            "   8.           3.           2.           2.          16.\n",
            "   3.           3.          24.           3.           3.\n",
            "   8.          16.           2.           4.           2.\n",
            "   8.           8.           6.97771588  16.           6.97771588\n",
            "   0.           8.           2.           3.           8.\n",
            "   6.97771588   0.           0.           8.           8.\n",
            "   8.           2.           4.           3.           4.\n",
            "   4.           4.           8.           8.           1.\n",
            " 120.           8.           4.           4.           2.\n",
            "  16.           2.           8.           3.           4.\n",
            "   1.           3.           2.           3.           8.\n",
            "   3.           8.           2.           1.           8.\n",
            "   3.           3.           3.           2.           4.\n",
            "   4.           0.          40.          24.           3.\n",
            "   4.           8.           2.           2.           2.\n",
            "   8.           2.           2.           1.           8.\n",
            "   2.           4.           8.           8.           8.\n",
            "   8.           4.           8.           8.           1.\n",
            "   2.         112.           1.           1.           8.\n",
            "   8.           8.           2.           1.           2.\n",
            "   4.           1.           4.           4.           8.\n",
            "   8.           4.           4.           8.          16.\n",
            "   4.           1.           5.           2.           3.\n",
            "   1.           1.           3.           2.           2.\n",
            "   8.           1.           4.           1.           2.\n",
            "   8.           8.           1.           3.           8.\n",
            "   3.           2.           2.           2.           1.\n",
            "   2.           8.           3.           4.           8.\n",
            "   3.           1.           1.           8.           1.\n",
            "   8.           3.           8.           8.           8.\n",
            "   0.           3.           1.           3.          24.\n",
            "   1.           8.           8.           8.           4.\n",
            "   8.           2.           2.           3.           1.\n",
            "   8.           8.           2.           0.           0.\n",
            "   4.           6.97771588   6.97771588   6.97771588   2.\n",
            "   6.97771588   1.           3.           1.           3.\n",
            "   3.           4.           2.           8.           8.\n",
            "  16.           2.           3.           2.          80.\n",
            "  24.          16.           2.           2.           3.\n",
            "   2.           8.           3.           2.           8.\n",
            "   2.           3.           8.           3.           2.\n",
            "   8.           3.           8.           2.           3.\n",
            "   2.           2.           2.           2.           2.\n",
            "   2.           8.           3.           3.           3.\n",
            "   2.           2.           3.           3.           2.\n",
            "   2.           8.           2.           5.           3.\n",
            "   2.           2.           2.           2.           2.\n",
            "   2.           2.           2.           2.           2.\n",
            "   3.           3.         112.           2.           2.\n",
            "   3.           2.           3.           3.           8.\n",
            "   8.           2.           3.           2.           4.\n",
            "   2.           3.           8.           2.           8.\n",
            "   2.           2.           3.           3.           2.\n",
            "   3.           3.           8.          24.           3.\n",
            "   3.           2.         104.           8.           8.\n",
            "   8.           8.           8.           8.           2.\n",
            "  24.           2.           3.           2.           2.\n",
            "   8.           2.           8.           3.           2.\n",
            "   4.           8.           2.           2.           8.\n",
            "   3.           2.           3.           8.           1.\n",
            "   2.           8.          64.           8.           2.\n",
            "   2.           3.           1.           0.           2.\n",
            "   0.           1.          48.           8.           8.\n",
            "   8.           3.           8.           2.           2.\n",
            "   2.           8.           2.           8.           8.\n",
            "   1.           8.           3.           8.           8.\n",
            "   8.          24.           8.           2.           0.\n",
            "   0.           3.           2.           2.           3.\n",
            "   3.           8.           2.           3.           3.\n",
            "   4.           2.           8.           4.         120.\n",
            "  16.           2.           8.           8.          80.\n",
            "   8.           4.           0.           0.           0.        ]\n",
            "Train R-squared: 0.3070026109971551\n",
            "Test R-squared: -0.11498132096028857\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Absenteeism_at_work_Project - Copy.csv')\n",
        "\n",
        "# Clean the \"Work load Average/day\" column by removing commas and converting to numeric\n",
        "df[\"Work load Average/day \"] = df[\"Work load Average/day \"].str.replace(',', '').astype(float)\n",
        "\n",
        "# Replace missing values with the mean of each column\n",
        "df.fillna(df.mean(), inplace=True)\n",
        "\n",
        "# Separate features and target variable\n",
        "x = df.iloc[:, :18].to_numpy()\n",
        "y = df[\"Absenteeism time in hours\"].to_numpy()\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features (optional, but often recommended for neural networks)\n",
        "scaler = StandardScaler()\n",
        "xtrain = scaler.fit_transform(xtrain)\n",
        "xtest = scaler.transform(xtest)\n",
        "\n",
        "# Build the neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu', input_shape=(xtrain.shape[1],)))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "model.fit(xtrain, ytrain, epochs=50, batch_size=32, validation_data=(xtest, ytest), verbose=2)\n",
        "\n",
        "# Make predictions\n",
        "ytrain_pred = model.predict(xtrain)\n",
        "ytest_pred = model.predict(xtest)\n",
        "\n",
        "# Evaluate the model\n",
        "train_r2 = r2_score(ytrain, ytrain_pred)\n",
        "test_r2 = r2_score(ytest, ytest_pred)\n",
        "print(x)\n",
        "print(y)\n",
        "print(f\"Train R-squared: {train_r2}\")\n",
        "print(f\"Test R-squared: {test_r2}\")\n"
      ]
    }
  ]
}